---
title: Classification of Wildtype and Mutant Zebrafish Brains via Computational Method
author:
  - name: Shuli Hu
    affiliation: Smith College
    email: \email{vhu@smith.edu}
  - name: Wencong Li
    affiliation: Smith College
    email: \email{liwencong1995@gmail.com}
  - name: Dejia Tang
    affiliation: Smith College
    email: \email{dtang@smith.edu}
  - name: Ji Young Yun
    affiliation: Smith College
    email: \email{jyun38@smith.edu}
address:
  - code: Smith College
    address: Statistical and Data Sciences, Northampton, MA
abstract: |
 
  
author_summary: |


bibliography: mybibfile.bib
output: rticles::plos_article
csl: plos.csl
---

_Text based on plos sample manuscript, see [http://journals.plos.org/ploscompbiol/s/latex](http://journals.plos.org/ploscompbiol/s/latex)_

# Introduction

## Landmark Analysis

## Programming languages used
### Python 
### R
### Git

# Literature Review
The SVM algorithm is a classification algorithm that provides state-of-the-art performance in a wide variety of application domains, image classification.  During the past few years, SVM has been applied very broadly within the field of computational biology especially in pattern recognition problems, including protein remote homology detection, microarray gene expressions analysis, prediction of protein-protein interactions, etc. 

In 1999, Jaakkola et al. ushered in stage 4 of the development of homology detection algorithms with a paper that garnered the “Best paper” award at the annual Intelligent Systems for Molecular Biology conference. Their primary insight was that additional accuracy can be obtained by modeling the difference between positive and negative examples. Because the homology task required discriminating between related and unrelated sequences, explicitly modeling the difference between these two sets of sequences yields an extremely powerful method. The algorithm described in that paper is called SVM-Fisher. 

The SVM-Fisher method (Jaakkola et al., 1999, 2000) couples an iterative HMM training scheme with the SVM. For any given family of related proteins, the HMM provides a kernel function. First, the HMM is trained on positive members of the... (to be continued)

# Data
43 Wildtypes
35 mutants

## Variables
Number of points
Median r
Alpha
Theta

## Tidy Data
The original landmarks data is a wide table containing the sample index and all the columns holding information regarding the minimum and maximum values of Alpha and Theta, number of points, median r value, and the type of sample for a particular sample in each landmark. However, because all of such variables were joined by underscores in the variable names, such as `-14.29_-4.76_-0.79_0.0_50_pts` or `-14.29_-4.76_-0.79_0.0_50_r` and the value in each cell refers to the median r value or number of points, it was very difficult to see what each column actually represented. The ideal format of the data set was to have the sample index, minimum and maximum Alpha, minimum and maximum Theta, number of points, median r, and type of sample each be its own column. Hence, three key functions were used from the tidyr package: gather, separate, and spread. The gather function separated the dataset into key and value pairs for each index. The key was the column name containing all essential information connected by underscores and the value included the number of points or median r value. Then, the separate function separated the result from the gather function divided the column connected by underscore into 5 different columns, named as `min_alpha`, `max_alpha`, `min_theta`, `max_theta`, and `ptsOrR`. This was added to the result of the gather function that contained the index and value of each cell, either median R or number of points. Afterwards, the spread function widened the already wide table by expanding the `ptsOrR` column by creating two columns, each column representing median R and the number of points. 

## Missing Value
Samples with missing values are eliminated by Supporting vector machine. For wedges that do not have any point in it, `median r` cannot be calculated, which means that these sample will be eliminated when running SVM. Wedges without points have biologically meanings, and we should not ignore these wedges in our model. In order to keep the wedges in our model, we need to artificially pick a `median r` value to replace the missing ones. Supporting vector machine is sensitive to outliers, so we cannot pick an `r` value that could become outliers. We decided to calculate the mean of `median r` for the nth landmark of all 78 samples, and then we replace the missing `median r` values with the mean.

# Supporting Vector Machine
## What is SVM?
(SH)
SVM’s have been proven to be a powerful algorithm for supervised clustering. A good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. 

###The optimization problem

- The function of the hyperplane:

$f(x) = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 $

if $f(x) = 0$: the observation is on the hyperplane. 

$ y * ( \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 )$ is the perpendicular distance from the ith observation to the hyperplane. If it's >0, the observation falls at the right side of the hyperplane and vice versa.


## Implementation
### Cross-Validation
(DT)
## Result
()
## Visulization
()

# Conclusion
()
## Discussion
()
Strength
Limitation

## Future Study
()
Improvement: 

Itenerating machine learning

Instead of cross-validation, better results (what kind of better results) could be achieved by using itenerating machine learning method. In iterative machine learning we repeat the process of training and testing several times. At the first round the user gives examples of objects belonging to some classes and the machine learning algorithm is trained with this data. In the second round, the algorithm shows examples of objects it thinks that belong to these classes. Now, the user merely adds objects to the improved training set which the machine learning algorithm has put into a wrong class. That is, the user only corrects the “misunderstandings” of the algorithm. In this way we can concentrate on difficult examples of objects that are hard to classify or are for some reason easily missed by humans. Such objects may lie close to the decision boundaries or in the periphery in the multidimensional feature space. This iterative process is continued until the machine learning algorithm does not make any mistakes or the classification results do not improve anymore.


Other models

# User Interface
## Input file 
Input file must contain landmark data. Variables that are needed for classification are required to be included in the input file.
In our analysis, we used number of points in each sub-section corresponding to each landmark of the 3D shape and the `median R` of points in each wedge.



# References {#references .unnumbered}

# Appendix
## Figures
## Python code


