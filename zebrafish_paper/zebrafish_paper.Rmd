---
title: Classification of Wildtype and Mutant Zebrafish Brains via Computational Method
author:
  - name: Shuli Hu
    affiliation: Smith College
    email: \email{vhu@smith.edu}
  - name: Wencong Li
    affiliation: Smith College
    email: \email{liwencong1995@gmail.com}
  - name: Dejia Tang
    affiliation: Smith College
    email: \email{dtang@smith.edu}
  - name: Ji Young Yun
    affiliation: Smith College
    email: \email{jyun38@smith.edu}
address:
  - code: Smith College
    address: Statistical and Data Sciences, Northampton, MA

abstract: Classification of biological creatures’ phenotypes has long been a field that scientists study at. In this project, we utilize support vector machine to distinguish structures of Zebrafish’s brains by using data generated from landmark analysis (cited Morgan’s paper). We create a tool for biologists to intuitively classify three-dimensional biological shapes into two groups, usually defined as wild type and mutant, and understand which part of the shapes have the most impact on the classification result. This project derives from Professor Barresi’s biological image analysis research at Smith College.
 
bibliography: mybibfile.bib
output: rticles::plos_article
csl: plos.csl
---

# Acknowledgements
This project was completed in partial fulfillment of the requirements of SDS 410: SDS Capstone. This course is offered by the Statistical and Data Sciences Program at Smith College, and was taught by Benjamin Baumer in Spring 2018.


# Introduction
This project derives from Professor Barresi’s biological image analysis research at Smith College and provides a tool to classify the structures within zebrafish brains via support vector machine. Our goal is to distinguish the wild and mutant types of zebrafish brain’s structures. Morgan, a student in Barresi Lab, used  landmarks analysis to divide the points in the three-dimentional images into small wedges and computed the landmark, which is the most representative point, within each wedge. The image of signals in a Zebrafish brain is shown in Figure 1. The shape is divided into 30 slices, and each slice is further divided into 8 wedges. The landmark in each wedge is calculated by taking the median distance of all points in each wedge, $R$. We use number of points in each wedge and median R to run SVM models to do classifications.  

## Landmark Analysis

## Programming languages used
### Python 
### R
### Git

# Literature Review
Research in developmental biology has relied on the analysis of morphological phenotypes through
qualitative examination of maximum intensity projections that surrender the power of three dimensional
data. Statistical methods to analyze visual data are needed, particularly to detect subtle
phenotypes. 

Morgan et al. (2018) have utilized the open source program, Ilastik, which employs a training based machine learning, to eliminate the image noise. Then they preformed principal component analysis to align commissures between samples, reducing misalignment artifacts, and implemented a cylindrical coordinate system which preserves image dimensionality normally lost in maximum intensity projection (MIP), which facilitates presentation of the data, but sacrifices much of the complexity and relational data contained in the image. Then they reduced the points identified by the program as belonging to the structure to a set of landmark points that describe the shape and distribution of signal corresponding to the structure. Finally, using the landmark system, we are able to identify and quantify structural differences and changes in signal distribution between wild type and mutant commissures.

Landmarks describe a shape by locating a finite number of points on each specimen. There are three basic types of landmarks: scientific, mathematic and pseudo-landmarks. A scientific landmark is a point assigned by an expert that corresponds between objects in some scientifically meaningful way, for example the corner of an eye. Mathematical landmarks are points located on an object according to some mathematical or geometrical property of the figure. Since it does not assume a preference of one location to another, it is particularly useful in automated morphological recognition and analysis for under-studied structure. Pseudo-landmarks are constructed points on an object, located either around the outline or in between scientific or mathematic landmarks. It is often used to approximate continuous curves (Dryden and Mardia, 2016). This research has chosen to calculate an automatic set of landmarks distributed across the structure in order to avoid introducing bias due to expectations about where biological differences should emerge.

Morgan et al. used Random Forest machine leaning method to classify the landmarks. Although the classification is quite accurate, it is difficult to interpret the result from biological aspects. Instead of doing classification on all of the landmarks at the same time, we decided to do classifacation on one landmark at a time via Support Vector Machine. The SVM algorithm is a classification algorithm that provides state-of-the-art performance in a wide variety of application domains, image classification. During the past few years, SVM has been applied very broadly within the field of computational biology especially in pattern recognition problems, including protein remote homology detection, microarray gene expressions analysis, prediction of protein-protein interactions, etc. 

In 1999, Jaakkola et al. ushered in stage 4 of the development of homology detection algorithms with a paper that garnered the “Best paper” award at the annual Intelligent Systems for Molecular Biology conference. Their primary insight was that additional accuracy can be obtained by modeling the difference between positive and negative examples. Because the homology task required discriminating between related and unrelated sequences, explicitly modeling the difference between these two sets of sequences yields an extremely powerful method. 

# Data
We have 43 wildtypes samples and 35 mutant samples for training and testing. There are 152 landmarks for each sample, with each of them containing the following variables:
number of points in that wedge
median R (micro-meter): the median of the distances to the center of the slice of all the points in that wedge. 
alpha (micro-meter): distance from the center of the landmark to the midline
theta (radian): the degree that shows the location of the wedge of a slice

We used the number of points and the median R to do classification via support vector machine. For missing ‘median R’ values due to absence of points in particular landmarks, we filled them with the median value of all the points in that landmark.


## Variables
Number of points
Median r
Alpha
Theta

## Tidy Data
The original landmarks data is a wide table containing the sample index and all the columns holding information regarding the minimum and maximum values of Alpha and Theta, number of points, median r value, and the type of sample for a particular sample in each landmark. However, because all of such variables were joined by underscores in the variable names, such as `-14.29_-4.76_-0.79_0.0_50_pts` or `-14.29_-4.76_-0.79_0.0_50_r` and the value in each cell refers to the median r value or number of points, it was very difficult to see what each column actually represented. The ideal format of the data set was to have the sample index, minimum and maximum Alpha, minimum and maximum Theta, number of points, median r, and type of sample each be its own column. Hence, three key functions were used from the tidyr package: gather, separate, and spread. The gather function separated the dataset into key and value pairs for each index. The key was the column name containing all essential information connected by underscores and the value included the number of points or median r value. Then, the separate function separated the result from the gather function divided the column connected by underscore into 5 different columns, named as `min_alpha`, `max_alpha`, `min_theta`, `max_theta`, and `ptsOrR`. This was added to the result of the gather function that contained the index and value of each cell, either median R or number of points. Afterwards, the spread function widened the already wide table by expanding the `ptsOrR` column by creating two columns, each column representing median R and the number of points. 

## Missing Value
Samples with missing values are eliminated by Supporting vector machine. For wedges that do not have any point in it, `median r` cannot be calculated, which means that these sample will be eliminated when running SVM. Wedges without points have biologically meanings, and we should not ignore these wedges in our model. In order to keep the wedges in our model, we need to artificially pick a `median r` value to replace the missing ones. Supporting vector machine is sensitive to outliers, so we cannot pick an `r` value that could become outliers. We decided to calculate the mean of `median r` for the nth landmark of all 78 samples, and then we replace the missing `median r` values with the mean.

# Supporting Vector Machine
<<<<<<< HEAD
SVM’s have been proven to be a powerful algorithm for supervised clustering. During the past few years, SVM has been applied very broadly within the field of computational biology especially in pattern recognition problems. The goal of SVM is to find a seperation line $f(x) = (\beta_0 + \beta_1 * x_1 + \beta_2 * x_2)$ that separates the nearest data as clean as possible. The parameters \beta are found by solving the optimization problem --to maximize M subject to some restrictions -- in 2 dimensions below. 
=======
SVM’s have been proven to be a powerful algorithm for supervised clustering. During the past few years, SVM has been applied very broadly within the field of computational biology especially in pattern recognition problems. The goal of SVM is to find a hyperplane $Y_i(b_0 + b_1X_{i1} + b_2X_{i2}) = 0$ that separates the nearest data as clean as possible. The parameters bi are found by solving the optimization problem below. 
>>>>>>> 556f192b4b54e80ee1cfd1aa3c036c34700bb88a

$\sum_{i=0}^n \beta_{i}^2$
$\beta_0 + \beta_1 * x_1 + \beta_2 * x_2 \geq M(1-\varepsilon_i)$
$\varepsilon_i \geq 0 $
$\sum_{i=0}^n \varepsilon_i \leq C$

<<<<<<< HEAD
=======
SVM’s have been proven to be a powerful algorithm for supervised clustering. During the past few years, SVM has been applied very broadly within the field of computational biology especially in pattern recognition problems. The goal of SVM is to find a hyperplane $Y_i(b_0 + b_1X_{i1} + b_2X_{i2}) = 0$ that separates the nearest data as clean as possible. The parameters bi are found by solving the optimization problem below. 
>>>>>>> 556f192b4b54e80ee1cfd1aa3c036c34700bb88a

* $C$: tuning parameter, toleration of violation.
* $M$: margin, distance of the closest points to the hyperplane.
* $e_i$ : slack variable, correct/incorrect classification of a point.

function of the seperation line:

$$f(x) = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 $$

if $f(x) = 0$, the observation is on the seperation line. 

$$ y * ( \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 )$$
is the perpendicular distance from the ith observation to the hyperplane. If it's >0, the observation falls at the right side of the speration line and vice versa.


# Workflow and User Interface
```{r fig.width=5, fig.height=5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("./figures/Figure1.png")
grid.raster(img)
```



## Step One: Data Processing and Modelling
This step is implemented using Python and packages including pandas, nump and sklearn are required. Users would need to run and interact with the Python script `svm.py` to build the model and pre-process the data if needed.   

The script `svm.py` contains two components: a general-purpose `svm_classification()` function that builds a SVM model to classify points for a perticular landmark and a `main()` function that runs the `svm_classification()` function for each landmark.

### User Interaction


### Input File 
Input file must contain landmark data. Variables that are needed for classification are required to be included in the input file.
In our analysis, we used number of points in each sub-section corresponding to each landmark of the 3D shape and the `median R` of points in each wedge.

### Sample input file

### Output File

## Step Two: Result Analyzation and Visualization

## Testing

### Cross-Validation
For our project, we have access to 43 wild-type samples and 35 mutant-type samples. Due to this limited sample size, we dicided to use a leave-one-out cross validation method to test our model.   
For each testing sample, we built 152 SVMs for each landmark.
For each SVM, we used 10-fold cross validation to select a tuning parameter C value among 0.1, 1 and 10.
After we get precision scores and predictions of each landmark, we will present the distribution of the landmarks’ precision scores (Figure 3).
The user would then be allowed to set a threshold for certain precision scores to select out a subset of landmarks that are considered significant.
A majority vote would then be performed among the selected landmarks to get an overall prediction for the sample.



## Result

## Visulization
()

# Discussion
## Strengths
The SVM model application on landmark data gives insightful analysis of:
which landmark (=which part of the brain) carries more information of the type.
whether a new sample is of mutant or wild type

<<<<<<< HEAD

## Discussion
()
In the previous method random forest, the number of predictors exceeds the number of samples. Morgan applied PCA do reduce the number of predictors. The problem with dimension reduction is that it gives a linear combination of the dimensions that are projected on those are kept. While the largest projections still make sense, the minor projections appear to be random and thus difficult to relate the results back to the structure of zebrafish's brain.

The SVM model also has its limitation in that it only considers one single landmark at a time without considering the relationship across the landmarks of the whole sample. However, it's easier to relate the result directly to the locations of the brain. 


## Future Study
We will continue to work on:
making the program more user friendly
running more tests to prove the accuracy of our model

### Improvement: 
=======
In the previous method random forest, the number of predictors p exceeds the number of samples. Morgan applied PCA do reduce the dimension of the predictors. The problem with dimension reduction is that it gives a linear combination of the dimensions that are projected on those are kept. While the largest projections still make sense, the minor projections are very random and thus difficult to interpret. 

## Limitations
The SVM model also has its limitation in that it only considers one single landmark at a time without considering the relationship across the landmarks of the whole sample.
>>>>>>> 556f192b4b54e80ee1cfd1aa3c036c34700bb88a

## Improvements 
Itenerating machine learning

Instead of cross-validation, better results could be achieved by using itenerating machine learning method. In iterative machine learning we repeat the process of training and testing several times. At the first round the user gives examples of objects belonging to some classes and the machine learning algorithm is trained with this data. In the second round, the algorithm shows examples of objects it thinks that belong to these classes. Now, the user merely adds objects to the improved training set which the machine learning algorithm has put into a wrong class. That is, the user only corrects the “misunderstandings” of the algorithm. In this way we can concentrate on difficult examples of objects that are hard to classify or are for some reason easily missed by humans. Such objects may lie close to the decision boundaries or in the periphery in the multidimensional feature space. This iterative process is continued until the machine learning algorithm does not make any mistakes or the classification results do not improve anymore. It will improve our classification results and thus is likely to help make better predictions for unknown type. 

<<<<<<< HEAD

Other models?
=======
Other models
>>>>>>> 556f192b4b54e80ee1cfd1aa3c036c34700bb88a

 

## Future Study
We will continue to work on:
making the program more user friendly
running more tests to prove the accuracy of our model


# Appendix Code
## Support Vector Machine

## Shiny App
```{r, eval=FALSE}
list_of_indices <- c(index$Index, "AT", "ZRF")
list_of_scores <- c("precision", "recall", "f1", "w_precision", "w_recall", "w_f1", "m_precision", "m_recall", "m_f1")
landmark_xy <- fread("/Users/priscilla/Desktop/SDS Capstone/Zebrafish/analysis/landmark_xy.csv")
list_of_channel <- c("AT", "ZRF")

# User Interface
ui <- fluidPage(
  titlePanel(title=h4("Classification of Wildtype and Mutant Zebrafish Brains via Computational Method", 
                      align="center")),
  selectInput("channel", "Channel:", list_of_channel),
  selectInput("sampleindex", "Sample Index:", list_of_indices),
  selectInput("score", "Accuracy Measurement:", list_of_scores),
  mainPanel(fluidRow(
              splitLayout(cellWidths = c("90%", "60%"), plotOutput("plot1"), plotOutput("plot2"))
            ))
)

# Server
server <- function(input,output) {
  dat <- reactive({
    dir <- paste0(wd, "/analysis/r", input$sampleindex, "_med_", input$channel, "_result.csv")
    test <- fread(dir)
    test <- test %>%
      left_join(landmark_xy, by="landmark_index")
    print(test)
    test
  })
  
  # Plot One
  output$plot1 <- renderPlot({
    p1 <- ggplot(dat(), 
                 aes(x = y, y = x)) +
      geom_tile(aes(fill = input$score)) +
      xlab("Alpha") +
      ylab("Theta") +
      scale_x_continuous(limits = c(1, 19), breaks=c(1, 10, 19), labels=c("-90.51", "0", "90.51")) +
      scale_y_continuous(limits = c(1, 8), breaks=c(1, 4.5, 8), labels=c("-3.14","0","3.14")) +
      scale_fill_continuous(limits=c(0, 1), breaks=seq(0,1,by=0.25)) 
    p1
  })
  
  # Plot Two
  output$plot2 <- renderPlot({
    p2 <- qplot(input$score, geom = "histogram") +
      xlab("Precision") +
      ylab("Count")  
    p2
  })
  
}
```

# References {#references .unnumbered}
