---
title: Classification of Wildtype and Mutant Zebrafish Brains via Computational Method
author:
  - name: Shuli Hu
    affiliation: Smith College
    email: \email{vhu@smith.edu}
  - name: Wencong Li
    affiliation: Smith College
    email: \email{liwencong1995@gmail.com}
  - name: Dejia Tang
    affiliation: Smith College
    email: \email{dtang@smith.edu}
  - name: Ji Young Yun
    affiliation: Smith College
    email: \email{jyun38@smith.edu}
address:
  - code: Smith College
    address: Statistical and Data Sciences, Northampton, MA
abstract: |
 
  
author_summary: |


bibliography: mybibfile.bib
output: rticles::plos_article
csl: plos.csl
---

_Text based on plos sample manuscript, see [http://journals.plos.org/ploscompbiol/s/latex](http://journals.plos.org/ploscompbiol/s/latex)_

# Introduction

## Landmark Analysis

## Programming languages used
### Python 
### R
### Git

# Literature Review
(Shuli)

# Data
xx Wildtypes
xx mutants

## Variables
Number of points
Median r
Alpha
Theta

## Tidy Data
The original landmarks data is a wide table containing the sample index and all the columns holding information regarding the minimum and maximum values of Alpha and Theta, number of points, median r value, and the type of sample for a particular sample in each landmark. However, because all of such variables were joined by underscores in the variable names, such as `-14.29_-4.76_-0.79_0.0_50_pts` or `-14.29_-4.76_-0.79_0.0_50_r` and the value in each cell refers to the median r value or number of points, it was very difficult to see what each column actually represented. The ideal format of the data set was to have the sample index, minimum and maximum Alpha, minimum and maximum Theta, number of points, median r, and type of sample each be its own column. Hence, three key functions were used from the tidyr package: gather, separate, and spread. The gather function separated the dataset into key and value pairs for each index. The key was the column name containing all essential information connected by underscores and the value included the number of points or median r value. Then, the separate function separated the result from the gather function divided the column connected by underscore into 5 different columns, named as `min_alpha`, `max_alpha`, `min_theta`, `max_theta`, and `ptsOrR`. This was added to the result of the gather function that contained the index and value of each cell, either median R or number of points. Afterwards, the spread function widened the already wide table by expanding the `ptsOrR` column by creating two columns, each column representing median R and the number of points. 

## Missing Value
Samples with missing values are eliminated by Supporting vector machine. For wedges that do not have any point in it, `median r` cannot be calculated, which means that these sample will be eliminated when running SVM. Wedges without points have biologically meanings, and we should not ignore these wedges in our model. In order to keep the wedges in our model, we need to artificially pick a `median r` value to replace the missing ones. Supporting vector machine is sensitive to outliers, so we cannot pick an `r` value that could become outliers. We decided to calculate the mean of `median r` for the nth landmark of all 52 samples, and then we replace the missing `median r` values with the mean.

# Supporting Vector Machine
## What is SVM?
(SH)
SVMâ€™s have been proven to be a powerful algorithm for supervised clustering. A good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. 

###The optimization problem

- The function of the hyperplane:

$f(x) = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 $

if $f(x) = 0$: the observation is on the hyperplane. 

$ y * ( \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 )$ is the perpendicular distance from the ith observation to the hyperplane. If it's >0, the observation falls at the right side of the hyperplane and vice versa.


## Implementation
### Cross-Validation
(DT)
## Result
()
## Visulization
()

# Conclusion
()
## Discussion
()
Strength
Limitation

## Future Study
()
Improvement
Other models



# References {#references .unnumbered}

# Appendix
## Figures
## Python code


